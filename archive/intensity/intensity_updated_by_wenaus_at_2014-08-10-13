{"tags": ["art", "fnal", "gaudi", "geant4", "genie", "neut", "root"], "refs": [], "refstothis": [], "othertags": ["genie", "neut"], "uses": [], "user": "wenaus", "logo": "", "usedby": [], "loggedin": true, "mytags": ["intensity"], "ent": {"web": null, "mytag": "intensity", "description": "<h4>Software and computing needs of the intensity frontier</h4>\n\n<ul>\n<li>Frameworks\n<ul>\n<li>The Fermilab-based IF experiments (from g-2, NOvA to LAr experiments including MicroBooNE and LBNE) have converged on ART as a framework for job control, I/O operations, and tracking of data provenance. </li>\n<li>Experiments outside of Fermilab (or before ART) use LHC derived frameworks such as Gaudi or homegrown frameworks like MINOS(+), IceTray and RAT.</li>\n</ul></li>\n<li>Software packages\n<ul>\n<li><a href=\"/e/root\">ROOT Data Analysis Framework</a> and <a href=\"/e/geant4\">Geant4</a> are the bread and butter of all HEP experiments. They are critical to all experiments in IF. Support for these packages is essential.\n<ul>\n<li>Geant4 has traditionally focused on EF experimental support. More ties/stronger support to IF experiments is a requirement. </li>\n<li>As an example, Geant4 is barely suitable for large scintillation detectors; given a complex geometry and large number of photons to track. </li>\n<li>Community desires improved efficiency for both of these packages. For example better ROOT I/O and Geant multi-threading (latter in beta?).</li>\n</ul></li>\n<li>Neutrino experiments use specialized packages for neutrino interactions: GENIE and Neut. GENIE is a public package that would benefit from continued support as it is heavily used in US experiments. <br>\n<ul>\n<li>Comprehensive simulation of neutrino-nucleus interactions with state-of-the-art generators (GENIE) has been added GEANT4.  </li>\n</ul></li>\n<li><a href=\"/e/larsoft\">LArSoft</a> is a common simulation, reconstruction and analysis toolkit for use by experiments using liquid argon time projection chambers (LArTPCs) that is managed by Fermilab. All US experiments using LArTPCs currently use LArSoft. Similarly the LAr and NOvA experiments share a simulation toolkit. \n<ul>\n<li>Joint efforts where possible make better use of development and maintenance resources. </li>\n</ul></li>\n<li>There are a number of other specialized physics packages in use by the community, for example: FLUKA for beam line simulations, CRY for simulating cosmic ray particles, NEST for determining ionization and light production in noble liquid detectors GLOBES for experiment design. \n<ul>\n<li>By no means a comprehensive list can be discussed here. More complete responses to the survey will be made available. </li>\n</ul></li>\n<li>Less specialized packages but equally important are those relating to infrastructure access for code management, data management, grid access, electronic log books and document management. Experiments differ much more on these than on the specialized ones (unless lab centralized).</li>\n</ul></li>\n<li>Access to dedicated resources\n<ul>\n<li>Hardware demands of IF experiments and IF R&amp;D modest compared to those of EF experiments. However the needs are NOT insignificant. \n<ul>\n<li>As an example, each Fermilab based IF experiment will require 1000s of dedicated batch slots. NOvA will need ~5M CPU hours to generate just simulation files. LBNE expects to use PB of storage, even smaller experiments like MicroBoone and MINERvA expect to use close to a PB. </li>\n</ul></li>\n<li>Efficient use of available grid resources has had/could have a huge impact on IF experiments and IF R&amp;D. \n<ul>\n<li>As an example experiments like T2K run intensively on grid resources in Europe and Canada. The US groups use a combination of local university and international resources (albeit with lower priority on the latter).</li>\n</ul></li>\n<li>Dedicated storage resources are needed for internationally or university run IF experiments as well as IF R&amp;D.\n<ul>\n<li>Data storage in T2K will also get to PB scale is distributed in Japan, Canada (Triumf), UK (RAL). SNO+ similarly uses Canada and UK grid storage. Slow link to the US for local analysis of data/simulations.</li>\n</ul></li>\n<li>Access to grid resources is all important and high on every experiment\u2019s list. </li>\n</ul></li>\n<li>Data handling and storage\n<ul>\n<li>The use of Fermigrid and Open-Science Grid are essential to all experiments responding to the survey.</li>\n<li>Fermilab-based experiments indicated that all data is stored on site. The infrastructure there handles active storage as well as archiving of data. SAM was noted as the preferred data distribution system for these experiments. Heavy I/O for analysis of large numbers of smaller sized events is an issue for systems like BlueArc.</li>\n<li>Professional support is required for methods to seamlessly use Fermilab and non-Fermilab resources through job submission protocols. \n<ul>\n<li>For Fermilab-based experiments university and other national lab resources are used in the production of Monte Carlo files. A common protocol to access these resources such as OSG is in the current plans.</li>\n<li>All international efforts would benefit from an ATLAS-like model where institutions can set up official, verified mirrors of data and simulations. For these experiments, only UK and Canada grid sites are available to store data.</li>\n</ul></li>\n</ul></li>\n<li>Computing model\n<ul>\n<li>We found  a high degree of commonality among the various experiments\u2019 computing models despite large differences in type of data analyzed, the scale of processing, or the specific workflows followed.</li>\n<li>The model is summarized as a traditional event driven analysis and Monte Carlo simulation using centralized data storage that are distributed to independent analysis jobs running in parallel on grid computing clusters. Peak usage can be 10x than planned usage. </li>\n<li>For large computing facilities such a Fermilab, it is useful to design a set of scalable solutions corresponding to these patterns, with associated toolkits that would allow access and monitoring. Provisioning an experiment or changing a computing model would then correspond to adjusting the scales in the appropriate processing units.</li>\n<li>Computing should be made transparent to the user, such that non-experts can perform any reasonable portion of the data handling and simulation. Moreover, all experiments would like to see computing become more distributed across sites. Users without a home lab or large institution require equal access to dedicated resources.</li>\n</ul></li>\n<li>Evolution of computing model\n<ul>\n<li>The evolution of the computing model follows several lines including taking advantage of new computing paradigms, like storage clouds, different cache schemes, GPU and multicore processing.\n<ul>\n<li>As regards computing technology, there is a concern that as the number of cores in CPUs increases, RAM capacity and memory bandwidth will not keep pace, causing the single-threaded batch processing model to be progressively less efficient on future systems unless special care is taken to design clusters with this use case in mind. </li>\n<li>There is no current significant use of multi-threading, since the main bottlenecks are GEANT4 (single-threaded) and file I/O. However there is interest in real parallelization at the level of ART for example. </li>\n<li>Greater availability of multi-core/GPU hardware in grid nodes would provide motivation for upgrading code to use it. For example currently we can only run GPU-accelerated code on local, custom-built systems.  A proposed example for GPU use included \u201crepeated frequent tasks like quick down-going cosmics identification for pre-reconstruction filtering\u201d.</li>\n</ul></li>\n</ul></li>\n</ul>", "created_at": "2014-08-10T13:15:19+00:00", "updated_at": "2014-08-10T13:15:19+00:00", "subtype": "", "location": "", "alltags": " art  fnal  gaudi  geant4  genie  intensity  neut  root ", "date": "2014-08-10T00:00:00+00:00", "description_markup": "#### Software and computing needs of the intensity frontier\n\n* Frameworks\n    * The Fermilab-based IF experiments (from g-2, NOvA to LAr experiments including MicroBooNE and LBNE) have converged on ART as a framework for job control, I/O operations, and tracking of data provenance. \n    * Experiments outside of Fermilab (or before ART) use LHC derived frameworks such as Gaudi or homegrown frameworks like MINOS(+), IceTray and RAT.\n* Software packages\n    * #ROOT and #Geant4 are the bread and butter of all HEP experiments. They are critical to all experiments in IF. Support for these packages is essential.\n        * Geant4 has traditionally focused on EF experimental support. More ties/stronger support to IF experiments is a requirement. \n        * As an example, Geant4 is barely suitable for large scintillation detectors; given a complex geometry and large number of photons to track. \n        * Community desires improved efficiency for both of these packages. For example better ROOT I/O and Geant multi-threading (latter in beta?).\n    * Neutrino experiments use specialized packages for neutrino interactions: GENIE and Neut. GENIE is a public package that would benefit from continued support as it is heavily used in US experiments.  \n        * Comprehensive simulation of neutrino-nucleus interactions with state-of-the-art generators (GENIE) has been added GEANT4.  \n    * #LArSoft is a common simulation, reconstruction and analysis toolkit for use by experiments using liquid argon time projection chambers (LArTPCs) that is managed by Fermilab. All US experiments using LArTPCs currently use LArSoft. Similarly the LAr and NOvA experiments share a simulation toolkit. \n        * Joint efforts where possible make better use of development and maintenance resources. \n    * There are a number of other specialized physics packages in use by the community, for example: FLUKA for beam line simulations, CRY for simulating cosmic ray particles, NEST for determining ionization and light production in noble liquid detectors GLOBES for experiment design. \n        * By no means a comprehensive list can be discussed here. More complete responses to the survey will be made available. \n    * Less specialized packages but equally important are those relating to infrastructure access for code management, data management, grid access, electronic log books and document management. Experiments differ much more on these than on the specialized ones (unless lab centralized).\n* Access to dedicated resources\n    * Hardware demands of IF experiments and IF R&D modest compared to those of EF experiments. However the needs are NOT insignificant. \n        * As an example, each Fermilab based IF experiment will require 1000s of dedicated batch slots. NOvA will need ~5M CPU hours to generate just simulation files. LBNE expects to use PB of storage, even smaller experiments like MicroBoone and MINERvA expect to use close to a PB. \n    * Efficient use of available grid resources has had/could have a huge impact on IF experiments and IF R&D. \n        * As an example experiments like T2K run intensively on grid resources in Europe and Canada. The US groups use a combination of local university and international resources (albeit with lower priority on the latter).\n    * Dedicated storage resources are needed for internationally or university run IF experiments as well as IF R&D.\n        * Data storage in T2K will also get to PB scale is distributed in Japan, Canada (Triumf), UK (RAL). SNO+ similarly uses Canada and UK grid storage. Slow link to the US for local analysis of data/simulations.\n    * Access to grid resources is all important and high on every experiment\u2019s list. \n* Data handling and storage\n    * The use of Fermigrid and Open-Science Grid are essential to all experiments responding to the survey.\n    * Fermilab-based experiments indicated that all data is stored on site. The infrastructure there handles active storage as well as archiving of data. SAM was noted as the preferred data distribution system for these experiments. Heavy I/O for analysis of large numbers of smaller sized events is an issue for systems like BlueArc.\n    * Professional support is required for methods to seamlessly use Fermilab and non-Fermilab resources through job submission protocols. \n        * For Fermilab-based experiments university and other national lab resources are used in the production of Monte Carlo files. A common protocol to access these resources such as OSG is in the current plans.\n        * All international efforts would benefit from an ATLAS-like model where institutions can set up official, verified mirrors of data and simulations. For these experiments, only UK and Canada grid sites are available to store data.\n* Computing model\n    * We found  a high degree of commonality among the various experiments\u2019 computing models despite large differences in type of data analyzed, the scale of processing, or the specific workflows followed.\n    * The model is summarized as a traditional event driven analysis and Monte Carlo simulation using centralized data storage that are distributed to independent analysis jobs running in parallel on grid computing clusters. Peak usage can be 10x than planned usage. \n    * For large computing facilities such a Fermilab, it is useful to design a set of scalable solutions corresponding to these patterns, with associated toolkits that would allow access and monitoring. Provisioning an experiment or changing a computing model would then correspond to adjusting the scales in the appropriate processing units.\n    * Computing should be made transparent to the user, such that non-experts can perform any reasonable portion of the data handling and simulation. Moreover, all experiments would like to see computing become more distributed across sites. Users without a home lab or large institution require equal access to dedicated resources.\n* Evolution of computing model\n    * The evolution of the computing model follows several lines including taking advantage of new computing paradigms, like storage clouds, different cache schemes, GPU and multicore processing.\n        * As regards computing technology, there is a concern that as the number of cores in CPUs increases, RAM capacity and memory bandwidth will not keep pace, causing the single-threaded batch processing model to be progressively less efficient on future systems unless special care is taken to design clusters with this use case in mind. \n        * There is no current significant use of multi-threading, since the main bottlenecks are GEANT4 (single-threaded) and file I/O. However there is interest in real parallelization at the level of ART for example. \n        * Greater availability of multi-core/GPU hardware in grid nodes would provide motivation for upgrading code to use it. For example currently we can only run GPU-accelerated code on local, custom-built systems.  A proposed example for GPU use included \u201crepeated frequent tasks like quick down-going cosmics identification for pre-reconstruction filtering\u201d.", "type": "definition", "id": 1315, "allmytags": " intensity ", "name": "Intensity Frontier"}, "tagname": "intensity", "image": {}}