{"tags": ["computing", "distsw", "facility"], "refs": [], "refstothis": [], "othertags": ["computing", "facility"], "uses": [{"description": "", "tagref": "jobexec", "created_at": "2014-08-10T13:15:19+00:00", "updated_at": "2014-08-10T13:15:19+00:00", "entity": "wms", "subtype": null, "entityref": null, "textref_original": null, "textref": "jobexec", "type": "uses", "id": 1985, "name": "Job Execution and Resource Provisioning"}], "user": "wenaus", "logo": "", "usedby": [], "loggedin": true, "mytags": ["wms"], "ent": {"web": null, "mytag": "wms", "description": "<ul>\n<li>LHC computing has always involved utilizing highly distributed, heterogeneous resources for its processing</li>\n<li>The future is even more challenging: more resources, more heterogeneous (HPCs, clouds, volunteer computing, \u2026), and the need to utilize diverse distributed resources opportunistically extends well beyond the LHC</li>\n<li>Processing is data intensive and at a large scale \u2013 ATLAS data volume currently 150 PB, processed on ~160k cores globally in steady state</li>\n<li>Operational demands must be low \u2013 manpower is scarce \u2013 requiring high automation, robustness, effective monitoring/diagnostics</li>\n<li>These requirements met through workload management systems</li>\n<li>Common tools include\n<ul>\n<li>HTCondor, the indispensable backbone. SE middleware used but not needed beyond providing a HTCondor endpoint.</li>\n<li>PanDA, the distributed production and analysis system of ATLAS, now being extended to other experiments and sciences (CMS, AMS, ALICE, ASCR/DOE-HEP BigPanDA project\u2026)</li>\n<li>glideinWMS, an extension over HTCondor allowing easy, uniform access to distributed HTCondor resources. Used by CMS, OSG Vos</li>\n</ul></li>\n<li>FTEs: Significant. PanDA draws on ~4-5 FTEs in ATLAS (3 FTEs on core PanDA) and the BigPanDA program currently supports 3 more.</li>\n<li>Future: an area of active evolutionary growth. Systems proven and hardened at the LHC being extended for Run 2 and generalized for take-up by other communities.</li>\n</ul>", "created_at": "2014-08-10T13:15:19+00:00", "updated_at": "2014-08-10T13:15:19+00:00", "subtype": "", "location": "", "alltags": " computing  distsw  facility  wms ", "date": "2014-08-10T00:00:00+00:00", "description_markup": "* LHC computing has always involved utilizing highly distributed, heterogeneous resources for its processing\n* The future is even more challenging: more resources, more heterogeneous (HPCs, clouds, volunteer computing, \u2026), and the need to utilize diverse distributed resources opportunistically extends well beyond the LHC\n* Processing is data intensive and at a large scale \u2013 ATLAS data volume currently 150 PB, processed on ~160k cores globally in steady state\n* Operational demands must be low \u2013 manpower is scarce \u2013 requiring high automation, robustness, effective monitoring/diagnostics\n* These requirements met through workload management systems\n* Common tools include\n    * HTCondor, the indispensable backbone. SE middleware used but not needed beyond providing a HTCondor endpoint.\n    * PanDA, the distributed production and analysis system of ATLAS, now being extended to other experiments and sciences (CMS, AMS, ALICE, ASCR/DOE-HEP BigPanDA project\u2026)\n    * glideinWMS, an extension over HTCondor allowing easy, uniform access to distributed HTCondor resources. Used by CMS, OSG Vos\n* FTEs: Significant. PanDA draws on ~4-5 FTEs in ATLAS (3 FTEs on core PanDA) and the BigPanDA program currently supports 3 more.\n* Future: an area of active evolutionary growth. Systems proven and hardened at the LHC being extended for Run 2 and generalized for take-up by other communities.", "type": "task", "id": 1278, "allmytags": " wms ", "name": "Workload Management"}, "tagname": "wms", "image": {}}